{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests\n",
    "\n",
    "adapted from Optimization notebook from Marc Joos: https://github.com/cea-irfu-sap/CEAPythonWorkshopForAstronomers/tree/master/10-Optimization-Marc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code testing** is a good practice as soon as you start developping anything. It is crucial for two main reasons:\n",
    "- ensure that what you coded behave as expected (through *unit tests* and *integration tests*);\n",
    "- ensure that you don't break what you previously did when you are in the development process (*regression tests*).\n",
    "\n",
    "We will focus solely on *unit testing*, meaning that we will test only the smallest testable unit of the program. In most cases, it means that we will test  Python functions and classes. *Unit testing* is sometimes referred as ***whitebox testing***, because you have to know the content of your source code to test it properly. It can be opposed to ***blackbox testing*** (as *integration tests*) where you will test the wole code with a test case, not knowing what is inside the code, but knowing what it should return on this peculiar case.\n",
    "\n",
    "You could always test things directly on your code with `assert`ions, but it is both unefficient and it would be a pain to actually run all the tests you would have written. To make the task easier, different modules and frameworks exist for testing; I will here only talk about `unittest`, which comes from the standard library, and `pytest`, that is slightly more user-friendly (and included in your Anaconda distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file calcpy.py\n",
    "import numpy as np\n",
    "\n",
    "f = lambda x: 4./(1. + x*x)\n",
    "\n",
    "def compPi(niter=1000):\n",
    "    h  = 1./niter\n",
    "    pi = 0.\n",
    "    for i in range(niter):\n",
    "        x   = h*(i - 0.5)\n",
    "        pi += f(x)\n",
    "    error = abs(np.arccos(-1.) - pi*h)/np.arccos(-1.)\n",
    "    return pi*h, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though `unittest` comes along in the standard library, it is a quite heavy machinery to use, it is very verbose. To test your code is great, to test it efficiently is even better. Thankfully, other libraries exist, like `pytest`. `pytest` is an efficient unit framework, simpler to use, more natural to use, and with less *boilerplate code*.\n",
    "\n",
    "Writing unit tests with pytest is as simple as making files that are named: `test_*.py` with functions in them called `test_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file test_pi.py\n",
    "import math\n",
    "from calcpy import compPi\n",
    "\n",
    "def test_result(niter=1000):\n",
    "    prec = 1./niter\n",
    "    pi, error = compPi(niter)\n",
    "    assert abs(math.pi - pi)/math.pi < prec\n",
    "    \n",
    "def test_prec_result(niter=1000):\n",
    "    prec = 1./niter\n",
    "    pi, error = compPi(niter)\n",
    "    assert error < prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!py.test ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some magic has to happen to make it works:\n",
    "- the file containing the tests should be named `test_*.py`;\n",
    "- the test functions should also be named `test_()`;\n",
    "- `assert`ions need to be made; these are automatically detected by `pytest` which analysed them, and deduce the proper tests to run.\n",
    "\n",
    "And for the failing tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file test_pi.py\n",
    "import math\n",
    "from calcpy import compPi\n",
    "\n",
    "\n",
    "def test_result(niter=1000):\n",
    "    prec = 1./niter\n",
    "    pi, error = compPi(niter)\n",
    "    assert abs(math.pi - pi)/math.pi < prec\n",
    "    \n",
    "def test_prec_result(niter=1000):\n",
    "    prec = 1./niter\n",
    "    pi, error = compPi(niter)\n",
    "    assert error < prec\n",
    "    \n",
    "def test_result_logical_error(niter=1000):\n",
    "    prec = 1./niter\n",
    "    pi, error = compPi(1j)\n",
    "    assert abs(math.pi - pi)/math.pi < prec\n",
    "    \n",
    "def test_prec_result_failure(niter=1000):\n",
    "    prec = 1./niter\n",
    "    pi, error = compPi(niter)\n",
    "    assert error > prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!py.test ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not distinguish between logical errors and test failure, but the output is highly readable and should help you anyway to get what went wrong with your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to go further..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unit tests:**\n",
    "- [Python documentation](https://docs.python.org/2/library/unittest.html)\n",
    "- [pytest documentation](http://pytest.org/latest/)\n",
    "- [unit tests in *Dive into Python*](http://www.diveintopython.net/unit_testing/index.html#roman.intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
